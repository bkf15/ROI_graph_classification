{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num data samples: 1613\n",
      "Total label distribution: [0, 0, 0, 0]\n",
      "Processing dataset...\n",
      "Done\n",
      "tensor([ 41.0000,   0.9364,   0.1752,  25.4127,   8.9171, 130.0000,   0.2528,\n",
      "          7.2252, 109.0000,  45.1470,   0.3154])\n"
     ]
    }
   ],
   "source": [
    "#read all the jsons and add them to Data list\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "json_dir= r\"C:\\Users\\Brian\\Desktop\\AIProject\\roi_graph_jsons\"\n",
    "data_list = []\n",
    "label_counts=[0,0,0,0]\n",
    "for roi_json in os.listdir(json_dir):\n",
    "    with open(os.path.join(json_dir, roi_json), 'r') as f:\n",
    "        roi_data = json.loads(f.read())\n",
    "        #Json has following fields:\n",
    "        #  name - name of the image\n",
    "        #  node_features - [num_nodes num_features] matrix of features for each node\n",
    "        #  dist_edge_table - [2 num_edges] edges defined by distance metric\n",
    "        #  labels - dict, which contains p1_label, p2_label, p3_label (roi_data[\"labels\"][\"p1_label\"]\n",
    "        \n",
    "        #process the labels. see the function for more details\n",
    "        label, conf = process_labels2(roi_data[\"labels\"])\n",
    "        if label == -1:\n",
    "            #print(\"Bad sample, no label. Discarding.\")\n",
    "            continue\n",
    "            \n",
    "#         for lbl in roi_data[\"labels\"]:\n",
    "#             l = roi_data[\"labels\"][lbl]\n",
    "#             ind = 0 if l == \"Normal Duct\" else 1 if l==\"Columnar\" else 2 if l==\"Flat Epithelial\" else 3 if l==\"ADH\" else -1\n",
    "#             label_counts[ind] += 1\n",
    "            \n",
    "        #normalize input variables in x\n",
    "        \n",
    "        #print(len(roi_data[\"node_features\"][0]))\n",
    "        data_obj = Data(x=torch.tensor(roi_data[\"node_features\"], dtype=torch.float),\n",
    "                        edge_table=torch.tensor(roi_data[\"dist_edge_table\"], dtype=torch.long).type(torch.LongTensor), \n",
    "#With confidence score  #y=torch.tensor([label,conf], dtype=torch.long))\n",
    "                        y=torch.tensor([label], dtype=torch.float))\n",
    "        data_list.append(data_obj)\n",
    "        \n",
    "\n",
    "print(\"Num data samples: \" + str(len(data_list))) #was 1759, but discarding bad samples, its now 1613\n",
    "print(\"Total label distribution: \" + str(label_counts))\n",
    "print(\"Processing dataset...\")\n",
    "dataset = HistGraphDataset(data_list)\n",
    "#dataset.process()\n",
    "t = random_split(dataset, [math.floor(len(dataset)*(1/3)), math.ceil(len(dataset)*(2/3))])\n",
    "test = t[0]\n",
    "train = t[1]\n",
    "test_loader = DataLoader(test, batch_size=1) #537\n",
    "train_loader = DataLoader(train, batch_size=1) #1076\n",
    "print(\"Done\")\n",
    "for i, d in enumerate(train_loader):\n",
    "    print(d.x[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_max_pool, global_mean_pool, global_sort_pool\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "#         self.conv1 = GCNConv(11, 11, normalize=True) #keep at 11 dimensions\n",
    "#         self.conv2 = GCNConv(11, 11, normalize=True)  \n",
    "\n",
    "        self.gcn_hd = 24\n",
    "        self.conv1 = GCNConv(11, self.gcn_hd, normalize=True) #keep at 11 dimensions\n",
    "        self.conv2 = GCNConv(self.gcn_hd, self.gcn_hd, normalize=True)  \n",
    "        \n",
    "        self.lin_hd = 12\n",
    "        self.lin1 = torch.nn.Linear(self.gcn_hd,self.lin_hd)                   #FC layers for classification\n",
    "        #self.lin2 = torch.nn.Linear(self.lin_hd, 1)        \n",
    "        self.lin2 = torch.nn.Linear(self.lin_hd, 4)                   #down to 4 layers for 4 classes\n",
    "        \n",
    "        #add confidence scores\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_table, b = data.x, data.edge_table, data.batch\n",
    "        #x is [num_nodes num_features(11)], edge_table is [2 num_edges]\n",
    "        \n",
    "        #first conv layer\n",
    "        x = self.conv1(x, edge_table) \n",
    "        x = F.relu(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "\n",
    "        #second conv layer\n",
    "        x = self.conv2(x, edge_table)\n",
    "        x = F.relu(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        #global pooling\n",
    "        # since batch size is 1, need to pass as batch LongTensor of zeros with size num_nodes\n",
    "        out = global_max_pool(x, b).type(torch.float)\n",
    "        #out = global_mean_pool(x, b).type(torch.float)\n",
    "        #print(x.shape)\n",
    "        #print(b.shape)\n",
    "        #out = global_sort_pool(x, b, 2).view(2,self.gcn_hd)\n",
    "        #print(\"After pooling: \" + str(out.shape))\n",
    "        #print(\"Y: \" + str(data.y))\n",
    "        #print()\n",
    "        \n",
    "        #now through linear layers\n",
    "        out = self.lin1(out)\n",
    "        #out = torch.tanh(out)\n",
    "        #out = F.softmax(out, dim=1)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.lin2(out)\n",
    "        #out = F.sigmoid(out)              #for binary\n",
    "        out = F.softmax(out, dim=1)      #for 4 way\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#define training / testing logic (for 1 epoch)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)    \n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        #batch size is 1, need to assign all nodes to a batch for global pooling\n",
    "        data.batch = torch.tensor(np.zeros([data.num_nodes])).type(torch.long)\n",
    "        one_hot = torch.tensor(np.zeros((1,4)),dtype=torch.float)\n",
    "        one_hot[0,int(data.y)] = 1\n",
    "        data.one_hot = one_hot\n",
    "        #data.weight = torch.tensor([0.25, 0.75])  #weigh high risk examples more\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        \n",
    "        #print(\"Out: \" + str(out.type()))\n",
    "        #print(\"Y: \" + str(data.y.type()))\n",
    "    \n",
    "        #loss = F.nll_loss(out, data.y)   #negative log liklihood loss, for 4 way\n",
    "        #loss = F.binary_cross_entropy(out, data.y)   #for binary\n",
    "        loss = F.binary_cross_entropy(out, data.one_hot)\n",
    "        #print(loss)\n",
    "        #print()\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "#for binary\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    preds = [0,0]\n",
    "    preds_list = []\n",
    "    y_test = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            #print(model(data))\n",
    "            #pred = model(data).max(dim=1)[1]\n",
    "            pred = 1 if model(data) >= 0.5 else 0\n",
    "            preds_list.append(pred)\n",
    "            preds[pred] += 1\n",
    "        correct += 1 if pred == data.y else 0\n",
    "        y_test.append(int(data.y.item()))\n",
    "    print(\"Predictons on test set: \" + str(preds))\n",
    "    return correct / len(loader.dataset), preds_list, y_test\n",
    "\n",
    "def test_4way(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    preds = [0,0,0,0]\n",
    "    preds_list = []\n",
    "    y_test = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            #print(model(data))\n",
    "            pred = model(data).max(dim=1)[1]\n",
    "            #pred = 1 if model(data) >= 0.5 else 0\n",
    "            preds_list.append(pred)\n",
    "            preds[pred] += 1\n",
    "        correct += 1 if pred == data.y else 0\n",
    "        y_test.append(int(data.y.item()))\n",
    "    print(\"Predictons on test set: \" + str(preds))\n",
    "    return correct / len(loader.dataset), preds_list, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictons on test set: [453, 0, 84, 0]\n",
      "Epoch 001, Loss: 2.5509, Test: 0.4842, F1: 0.0000\n",
      "Predictons on test set: [467, 0, 70, 0]\n",
      "Epoch 002, Loss: 1.9144, Test: 0.4991, F1: 0.0000\n",
      "Predictons on test set: [535, 0, 2, 0]\n",
      "Epoch 003, Loss: 1.3255, Test: 0.5475, F1: 0.0000\n",
      "Predictons on test set: [504, 28, 5, 0]\n",
      "Epoch 004, Loss: 0.7465, Test: 0.5345, F1: 0.0000\n",
      "Predictons on test set: [511, 20, 6, 0]\n",
      "Epoch 005, Loss: 0.6237, Test: 0.5363, F1: 0.0000\n",
      "Predictons on test set: [511, 16, 10, 0]\n",
      "Epoch 006, Loss: 0.5694, Test: 0.5363, F1: 0.0000\n",
      "Predictons on test set: [513, 10, 14, 0]\n",
      "Epoch 007, Loss: 0.4910, Test: 0.5400, F1: 0.0000\n",
      "Predictons on test set: [508, 4, 5, 20]\n",
      "Epoch 008, Loss: 0.3786, Test: 0.5326, F1: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7185b42ab585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#test_acc, y_pred, y_test = test(test_loader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_4way\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-297121f3266e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loader)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m#print()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\anaconda3\\envs\\pytorch_geometric\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brian\\anaconda3\\envs\\pytorch_geometric\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#actually running the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(1, 50):\n",
    "    loss = train(train_loader)\n",
    "    #test_acc, y_pred, y_test = test(test_loader)\n",
    "    test_acc, y_pred, y_test = test_4way(test_loader)\n",
    "    print('Epoch {:03d}, Loss: {:.4f}, Test: {:.4f}, F1: {:.4f}'.format(\n",
    "        epoch, loss, test_acc, 0))\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vals = [int(y.item()) for y in y_pred]\n",
    "print(classification_report(y_test,y_pred_vals))\n",
    "print(confusion_matrix(y_test,y_pred_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the label as a majority vote with confidence value. \n",
    "# if all 3 labels the same, return the label with confidence 1\n",
    "# if 2 agree on a label, return that label with confidence 0.66\n",
    "# if all 3 disagree, return the HIGHEST RISK LABEL with confidence 0.33\n",
    "# if any of them put \"dont know\", subtract from confidence \n",
    "# if all 3 are don't know, throw out example\n",
    "#\n",
    "# additionally, convert labels to numericals, with the following order:\n",
    "# 0: normal duct\n",
    "# 1: columnar\n",
    "# 2: flat epithelial\n",
    "# 3: adh\n",
    "#\n",
    "# returns [label, confidence]\n",
    "def process_labels(o_labels):\n",
    "    l1 = o_labels[\"p1_label\"]\n",
    "    l2 = o_labels[\"p2_label\"]\n",
    "    l3 = o_labels[\"p3_label\"]\n",
    "    mf_label = max(set([l1, l2, l3]), key = [l1, l2, l3].count)\n",
    "    ####all 3 agree######\n",
    "    if [l1, l2, l3].count(mf_label) == 3:\n",
    "        if mf_label == \"Normal Duct\":\n",
    "            return 0, 1\n",
    "        elif mf_label == \"Columnar\":\n",
    "            return 1, 1\n",
    "        elif mf_label == \"Flat Epithelial\":\n",
    "            return 2, 1\n",
    "        elif mf_label == \"ADH\":\n",
    "            return 3, 1\n",
    "        else:               #all \"Don't know\" / \"other\", throw out\n",
    "            return -1, 0\n",
    "        \n",
    "    ####only 2 agree######\n",
    "    elif [l1, l2, l3].count(mf_label) == 2:\n",
    "        if mf_label == \"Normal Duct\":\n",
    "            return 0, 0.66\n",
    "        elif mf_label == \"Columnar\":\n",
    "            return 1, 0.66\n",
    "        elif mf_label == \"Flat Epithelial\":\n",
    "            return 2, 0.66\n",
    "        elif mf_label == \"ADH\":\n",
    "            return 3, 0.66\n",
    "        else:               #2 votes for don't know / other, use other label with confidence 0.33\n",
    "            other_label = min(set([l1, l2, l3]), key = [l1, l2, l3].count)\n",
    "            if other_label == \"Normal Duct\":\n",
    "                return 0, 0.33\n",
    "            elif other_label == \"Columnar\":\n",
    "                return 1, 0.33\n",
    "            elif other_label == \"Flat Epithelial\":\n",
    "                return 2, 0.33\n",
    "            elif other_label == \"ADH\":\n",
    "                return 3, 0.33\n",
    "            else:               #2 votes for \"dont know\"/\"other\", the other vote for the other\n",
    "                return -1, 0\n",
    "    \n",
    "    ####they all disagree#####\n",
    "    else:  \n",
    "        #choose most severe diagnosis, use low confidence\n",
    "        if \"ADH\" in [l1, l2, l3]:\n",
    "            return 3, 0.33\n",
    "        elif \"Flat Epithelial\" in [l1, l2, l3]:\n",
    "            return 2, 0.33\n",
    "        elif \"Columnar\" in [l1, l2, l3]:\n",
    "            return 1, 0.33\n",
    "        elif \"Normal Duct\" in [l1, l2, l3]:\n",
    "            return 0, 0.33\n",
    "        else:\n",
    "            return -1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other option, just always take the most severe diagnosis, use count for confidence (hopefully get more high risk examples)\n",
    "def process_labels2(o_labels):\n",
    "    l1 = o_labels[\"p1_label\"]\n",
    "    l2 = o_labels[\"p2_label\"]\n",
    "    l3 = o_labels[\"p3_label\"]\n",
    "    l = [l1, l2, l3]\n",
    "    #choose most severe diagnosis\n",
    "    if \"ADH\" in l:\n",
    "        if l.count(\"ADH\") == 3:\n",
    "            return torch.tensor([3, 1])\n",
    "        elif l.count(\"ADH\") == 2:\n",
    "            return torch.tensor([3, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([3, 0.33])\n",
    "\n",
    "    elif \"Flat Epithelial\" in l:\n",
    "        if l.count(\"Flat Epithelial\") == 3:\n",
    "            return torch.tensor([2, 1])\n",
    "        elif l.count(\"Flat Epithelial\") == 2:\n",
    "            return torch.tensor([2, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([2, 0.33])\n",
    "\n",
    "    elif \"Columnar\" in l:\n",
    "        if l.count(\"Columnar\") == 3:\n",
    "            return torch.tensor([1, 1])\n",
    "        elif l.count(\"Columnar\") == 2:\n",
    "            return torch.tensor([1, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([1, 0.33])\n",
    "\n",
    "    elif \"Normal Duct\" in l:\n",
    "        if l.count(\"Normal Duct\") == 3:\n",
    "            return torch.tensor([0, 1])\n",
    "        elif l.count(\"Normal Duct\") == 2:\n",
    "            return torch.tensor([0, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([0, 0.33])     \n",
    "\n",
    "    else:\n",
    "        return torch.tensor([-1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as process_labels2, but a binary problem, just high and low risk\n",
    "def process_labels3(o_labels):\n",
    "    l1 = o_labels[\"p1_label\"]\n",
    "    l2 = o_labels[\"p2_label\"]\n",
    "    l3 = o_labels[\"p3_label\"]\n",
    "    l = [l1, l2, l3]\n",
    "    #choose most severe diagnosis\n",
    "    if \"ADH\" in l:\n",
    "        if l.count(\"ADH\") == 3:\n",
    "            return torch.tensor([1, 1])\n",
    "        elif l.count(\"ADH\") == 2:\n",
    "            return torch.tensor([1, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([1, 0.33])\n",
    "\n",
    "    elif \"Flat Epithelial\" in l:\n",
    "        if l.count(\"Flat Epithelial\") == 3:\n",
    "            return torch.tensor([1, 1])\n",
    "        elif l.count(\"Flat Epithelial\") == 2:\n",
    "            return torch.tensor([1, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([1, 0.33])\n",
    "\n",
    "    elif \"Columnar\" in l:\n",
    "        if l.count(\"Columnar\") == 3:\n",
    "            return torch.tensor([0, 1])\n",
    "        elif l.count(\"Columnar\") == 2:\n",
    "            return torch.tensor([0, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([0, 0.33])\n",
    "\n",
    "    elif \"Normal Duct\" in l:\n",
    "        if l.count(\"Normal Duct\") == 3:\n",
    "            return torch.tensor([0, 1])\n",
    "        elif l.count(\"Normal Duct\") == 2:\n",
    "            return torch.tensor([0, 0.66])\n",
    "        else:\n",
    "            return torch.tensor([0, 0.33])     \n",
    "\n",
    "    else:\n",
    "        return torch.tensor([-1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from sklearn.preprocessing import normalize\n",
    "class HistGraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "        self.num_classes = 2\n",
    "        #self.num_features = len(data_list[0].x[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    #normalize input variables x in data_list across EACH SAMPLE\n",
    "    #ie each sample is normalized relative to itself, might account for zoom issues\n",
    "    def process(self):\n",
    "        #normalize \n",
    "        for i, data in enumerate(self.data_list):\n",
    "            #print(\"After: \" + str(normalize(data.x,axis=0)[0]))\n",
    "            #self.data_list[i].x = torch.tensor(normalize(data.x, axis=1), dtype=torch.float)\n",
    "            self.data_list[i].x = torch.tensor(data.x / data.x.max(0, keepdim=True)[0], dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            print(\"IDX WAS TENSOR\")\n",
    "            idx = idx.tolist()\n",
    "        sample = self.data_list[idx]     \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-efccc15f07e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#class distributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "a = [0,0,0,0]\n",
    "for data in train_loader:\n",
    "    a[data.y[0].item()] = a[data.y[0].item()] + 1\n",
    "print(a) #class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
